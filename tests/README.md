# CogniBot Test Suite

This directory contains comprehensive tests for the CogniBot cognitive bias detection system.

## Test Structure

```
tests/
â”œâ”€â”€ __init__.py                 # Package initialization
â”œâ”€â”€ conftest.py                # Shared pytest fixtures and configuration
â”œâ”€â”€ pytest.ini                # Pytest configuration
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ test_bias_detector.py      # Unit tests for pattern-based bias detection
â”œâ”€â”€ test_llm_analyzer.py       # Unit tests for LLM-based analysis
â”œâ”€â”€ test_multilingual.py       # Tests for multilingual support (including Russian test)
â”œâ”€â”€ test_integration.py        # Integration tests for component interaction
â”œâ”€â”€ test_legacy.py             # Legacy test script (migrated from src/)
â””â”€â”€ run_russian_test.py        # Quick runner for Russian logical fallacy test
```

## Running Tests

### Prerequisites

```bash
# Install testing dependencies
pip install pytest pytest-asyncio

# Set up your .env file with OpenAI API key (for LLM tests)
cp src/env_template.txt .env
# Edit .env and add your OPENAI_API_KEY
```

### Test Commands

```bash
# Run all tests
pytest

# Run only unit tests (fast, no API calls)
pytest -m "not integration and not slow"

# Run with verbose output
pytest -v

# Run specific test file
pytest tests/test_bias_detector.py

# Run the Russian logical fallacy test
pytest tests/test_multilingual.py::TestMultilingualSupport::test_russian_logical_fallacy -v

# Run legacy test script (direct execution)
python tests/test_legacy.py

# Quick Russian test runner
python tests/run_russian_test.py
```

## Test Categories

### ğŸ§ª Unit Tests
- **test_bias_detector.py**: Tests pattern-based bias detection
- **test_llm_analyzer.py**: Tests LLM analysis functionality
- Fast execution, no external dependencies

### ğŸŒ Integration Tests  
- **test_integration.py**: Tests component interaction
- **test_multilingual.py**: Tests multilingual support including Russian
- Requires OpenAI API key for LLM analysis

### ğŸ“Š Test Markers

- `@pytest.mark.unit`: Fast unit tests
- `@pytest.mark.integration`: Tests requiring external services (OpenAI API)
- `@pytest.mark.slow`: Longer-running tests
- `@pytest.mark.asyncio`: Async tests (required for LLM analysis)

## Key Test Cases

### Russian Logical Fallacy
```
Text: "Ğ’ÑÑĞºĞ°Ñ ÑĞµĞ»ĞµĞ´ĞºĞ° Ñ€Ñ‹Ğ±Ğ°, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ Ğ²ÑÑĞºĞ°Ñ Ñ€Ñ‹Ğ±Ğ° - ÑĞµĞ»ĞµĞ´ĞºĞ°"
Translation: "Every herring is a fish, therefore every fish is a herring"
Expected: Logical fallacy (affirming the consequent)
```

### English Bias Examples
- **Ad Hominem**: "You're clearly an idiot if you believe that"
- **Bandwagon**: "Everyone knows this is true"
- **False Dichotomy**: "You're either with us or against us"
- **Strawman**: "So you're saying we should just give up completely?"

## Configuration

Tests use `conftest.py` for shared fixtures and `pytest.ini` for configuration:

- Automatic test discovery
- Async test support
- Warning filters
- Test markers for categorization

## CI/CD Ready

This test structure is ready for continuous integration:

```yaml
# Example GitHub Actions
- name: Run tests
  run: |
    pip install pytest pytest-asyncio
    pytest -m "not integration"  # Skip API-dependent tests in CI
```

## Development Workflow

1. **Write tests first** (TDD approach)
2. **Run unit tests frequently** during development
3. **Run integration tests** before commits
4. **Use markers** to control which tests run in different environments

## Notes

- Tests requiring OpenAI API will be skipped if no API key is provided
- Pattern-based tests run without external dependencies
- Fixtures in `conftest.py` provide reusable test data and mocks